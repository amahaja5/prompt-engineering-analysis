{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Engineering Analysis with Anthropic's Claude\n",
    "\n",
    "This notebook analyzes how different prompt engineering techniques affect Claude's output distributions, response characteristics, and behavior.\n",
    "\n",
    "## Topics Covered:\n",
    "1. Setting up the Anthropic API\n",
    "2. Comparing different prompt engineering techniques\n",
    "3. Analyzing temperature effects on output distribution\n",
    "4. Visualizing output diversity and characteristics\n",
    "5. Statistical analysis of prompt effectiveness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\".key\", 'r') as fp:\n",
    "    API_KEY = fp.read()\n",
    "MODEL_NAME = \"claude-3-haiku-20240307\"\n",
    "\n",
    "# Stores the API_KEY & MODEL_NAME variables for use across notebooks within the IPython store\n",
    "%store API_KEY\n",
    "%store MODEL_NAME\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import anthropic\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import List, Dict, Any\n",
    "from collections import Counter\n",
    "import time\n",
    "\n",
    "# Set style for better visualizations\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "# Import our analyzer\n",
    "from prompt_engineering_analyzer import PromptEngineeringAnalyzer\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure API Key\n",
    "\n",
    "Make sure you have your Anthropic API key set as an environment variable:\n",
    "```bash\n",
    "export ANTHROPIC_API_KEY='your-api-key-here'\n",
    "```\n",
    "\n",
    "Or set it directly in this notebook (not recommended for production):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Use environment variable (recommended)\n",
    "# Option 2: Set directly (uncomment and use with caution)\n",
    "api_key = API_KEY\n",
    "\n",
    "if not API_KEY:\n",
    "    print(\"⚠️ WARNING: ANTHROPIC_API_KEY not found!\")\n",
    "    print(\"Please set your API key before proceeding.\")\n",
    "else:\n",
    "    print(\"✓ API key configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the analyzer with your preferred model\n",
    "analyzer = PromptEngineeringAnalyzer(\n",
    "    api_key=api_key,\n",
    "    model=MODEL_NAME # or \"claude-3-opus-20240229\", etc.\n",
    ")\n",
    "\n",
    "print(f\"Analyzer initialized with model: {analyzer.model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basic Prompt Testing\n",
    "\n",
    "Let's start with a simple example to see how Claude responds to a basic prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test a simple prompt\n",
    "test_prompt = \"Explain what a neural network is in one sentence.\"\n",
    "\n",
    "response = analyzer.get_response(test_prompt, temperature=0.7)\n",
    "\n",
    "print(\"Prompt:\", test_prompt)\n",
    "print(\"\\nResponse:\", response['response_text'])\n",
    "print(\"\\nMetadata:\")\n",
    "print(f\"  - Input tokens: {response['input_tokens']}\")\n",
    "print(f\"  - Output tokens: {response['output_tokens']}\")\n",
    "#print(f\"  - Response length: {response['response_length']} characters\")\n",
    "print(f\"  - Stop reason: {response['stop_reason']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Comparing Prompt Engineering Techniques\n",
    "\n",
    "Now let's compare different prompt engineering approaches on the same question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your question\n",
    "base_question = \"What are the three most important factors in training a deep learning model?\"\n",
    "\n",
    "# Define different prompt engineering variants\n",
    "prompt_variants = {\n",
    "    \"baseline\": {\n",
    "        \"prompt\": base_question,\n",
    "        \"system\": \"\"\n",
    "    },\n",
    "    \n",
    "    \"with_system_prompt\": {\n",
    "        \"prompt\": base_question,\n",
    "        \"system\": \"You are an expert deep learning researcher with 10 years of experience.\"\n",
    "    },\n",
    "    \n",
    "    \"chain_of_thought\": {\n",
    "        \"prompt\": f\"{base_question}\\n\\nLet's think through this step by step:\",\n",
    "        \"system\": \"\"\n",
    "    },\n",
    "    \n",
    "    \"structured\": {\n",
    "        \"prompt\": f\"{base_question}\\n\\nProvide your answer in this format:\\n1. [Factor]: [Explanation]\\n2. [Factor]: [Explanation]\\n3. [Factor]: [Explanation]\",\n",
    "        \"system\": \"\"\n",
    "    },\n",
    "    \n",
    "    \"few_shot\": {\n",
    "        \"prompt\": f\"\"\"Here's an example of explaining key factors:\n",
    "\n",
    "Q: What are the three most important factors in building a web application?\n",
    "A: The three most important factors are:\n",
    "1. Security - Protecting user data and preventing vulnerabilities\n",
    "2. Performance - Ensuring fast load times and responsive interactions\n",
    "3. User Experience - Creating an intuitive and accessible interface\n",
    "\n",
    "Q: {base_question}\n",
    "A:\"\"\",\n",
    "        \"system\": \"\"\n",
    "    },\n",
    "    \n",
    "    \"role_playing\": {\n",
    "        \"prompt\": base_question,\n",
    "        \"system\": \"You are Yann LeCun, discussing deep learning with a colleague. Be concise but insightful.\"\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Prompt variants defined:\")\n",
    "for name in prompt_variants.keys():\n",
    "    print(f\"  - {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the comparison (this will take a few minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all variants\n",
    "print(\"Running comparison... This may take a few minutes.\\n\")\n",
    "\n",
    "df_variants = analyzer.compare_prompts(\n",
    "    base_question=base_question,\n",
    "    prompt_variants=prompt_variants,\n",
    "    temperature=0.7,\n",
    "    num_samples=5  # 5 samples per variant\n",
    ")\n",
    "\n",
    "print(\"\\n✓ Comparison complete!\")\n",
    "print(f\"\\nTotal responses collected: {len(df_variants)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View sample responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display one sample from each variant\n",
    "for variant in df_variants['variant'].unique():\n",
    "    sample = df_variants[df_variants['variant'] == variant].iloc[0]\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Variant: {variant}\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Response:\\n{sample['response_text']}\")\n",
    "    print(f\"\\nTokens: {sample['output_tokens']} | Length: {sample['response_length']} chars\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze output diversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diversity_metrics = analyzer.analyze_output_diversity(df_variants)\n",
    "\n",
    "# Convert to DataFrame for better display\n",
    "diversity_df = pd.DataFrame(diversity_metrics).T\n",
    "diversity_df = diversity_df.round(3)\n",
    "\n",
    "print(\"Output Diversity Metrics:\\n\")\n",
    "print(diversity_df)\n",
    "\n",
    "# Highlight key insights\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"KEY INSIGHTS:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "most_diverse = diversity_df['uniqueness_ratio'].idxmax()\n",
    "least_diverse = diversity_df['uniqueness_ratio'].idxmin()\n",
    "\n",
    "print(f\"\\nMost diverse responses: {most_diverse} (ratio: {diversity_df.loc[most_diverse, 'uniqueness_ratio']:.3f})\")\n",
    "print(f\"Least diverse responses: {least_diverse} (ratio: {diversity_df.loc[least_diverse, 'uniqueness_ratio']:.3f})\")\n",
    "\n",
    "longest = diversity_df['avg_response_length'].idxmax()\n",
    "shortest = diversity_df['avg_response_length'].idxmin()\n",
    "\n",
    "print(f\"\\nLongest responses: {longest} ({diversity_df.loc[longest, 'avg_response_length']:.0f} chars avg)\")\n",
    "print(f\"Shortest responses: {shortest} ({diversity_df.loc[shortest, 'avg_response_length']:.0f} chars avg)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer.visualize_prompt_comparison(df_variants, save_path=\"prompt_comparison.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Temperature Analysis\n",
    "\n",
    "Now let's see how temperature affects the output distribution for a single prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a prompt for temperature analysis\n",
    "temp_test_prompt = \"Name three creative uses for artificial intelligence in healthcare.\"\n",
    "\n",
    "print(\"Running temperature analysis...\\n\")\n",
    "print(f\"Prompt: {temp_test_prompt}\")\n",
    "print(f\"Temperatures to test: [0.0, 0.3, 0.7, 1.0, 1.5]\\n\")\n",
    "\n",
    "df_temperature = analyzer.analyze_temperature_effects(\n",
    "    prompt=temp_test_prompt,\n",
    "    system=\"You are a creative AI assistant.\",\n",
    "    temperatures=[0.0, 0.3, 0.7, 1.0],\n",
    "    num_samples=10\n",
    ")\n",
    "\n",
    "print(\"\\n✓ Temperature analysis complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View responses at different temperatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show one example from each temperature\n",
    "for temp in sorted(df_temperature['temperature'].unique()):\n",
    "    sample = df_temperature[df_temperature['temperature'] == temp].iloc[0]\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Temperature: {temp}\")\n",
    "    print(\"=\"*80)\n",
    "    print(sample['response_text'])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temperature statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate statistics by temperature\n",
    "temp_stats = df_temperature.groupby('temperature').agg({\n",
    "    'response_length': ['mean', 'std', 'min', 'max'],\n",
    "    'word_count': ['mean', 'std'],\n",
    "    'output_tokens': ['mean', 'std']\n",
    "}).round(2)\n",
    "\n",
    "print(\"Statistics by Temperature:\\n\")\n",
    "print(temp_stats)\n",
    "\n",
    "# Calculate uniqueness ratio by temperature\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Response Uniqueness by Temperature:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for temp in sorted(df_temperature['temperature'].unique()):\n",
    "    responses = df_temperature[df_temperature['temperature'] == temp]['response_text'].tolist()\n",
    "    unique_ratio = len(set(responses)) / len(responses)\n",
    "    print(f\"Temperature {temp}: {unique_ratio:.2%} unique responses ({len(set(responses))}/{len(responses)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize temperature effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer.visualize_temperature_effects(df_temperature, save_path=\"temperature_effects.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Custom Analysis: Response Patterns\n",
    "\n",
    "Let's do some custom analysis on common patterns in the responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_response_patterns(df, text_column='response_text'):\n",
    "    \"\"\"\n",
    "    Analyze common patterns in responses.\n",
    "    \"\"\"\n",
    "    patterns = {\n",
    "        'starts_with_number': 0,\n",
    "        'contains_list': 0,\n",
    "        'contains_bullet': 0,\n",
    "        'starts_with_capital': 0,\n",
    "        'contains_colon': 0,\n",
    "    }\n",
    "    \n",
    "    for text in df[text_column]:\n",
    "        if text[0].isdigit():\n",
    "            patterns['starts_with_number'] += 1\n",
    "        if any(text.startswith(f\"{i}.\") or f\"\\n{i}.\" in text for i in range(1, 10)):\n",
    "            patterns['contains_list'] += 1\n",
    "        if '•' in text or '- ' in text or '* ' in text:\n",
    "            patterns['contains_bullet'] += 1\n",
    "        if text[0].isupper():\n",
    "            patterns['starts_with_capital'] += 1\n",
    "        if ':' in text:\n",
    "            patterns['contains_colon'] += 1\n",
    "    \n",
    "    # Convert to percentages\n",
    "    total = len(df)\n",
    "    for key in patterns:\n",
    "        patterns[key] = (patterns[key] / total) * 100\n",
    "    \n",
    "    return patterns\n",
    "\n",
    "# Analyze patterns by variant\n",
    "print(\"Response Patterns by Variant:\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for variant in df_variants['variant'].unique():\n",
    "    variant_df = df_variants[df_variants['variant'] == variant]\n",
    "    patterns = analyze_response_patterns(variant_df)\n",
    "    \n",
    "    print(f\"\\n{variant}:\")\n",
    "    for pattern, percentage in patterns.items():\n",
    "        print(f\"  {pattern}: {percentage:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Word Frequency Analysis\n",
    "\n",
    "Analyze which words appear most frequently in different prompt variants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "def get_top_words(df, variant, n=20, min_length=4):\n",
    "    \"\"\"\n",
    "    Get top N words for a specific variant.\n",
    "    \"\"\"\n",
    "    # Get all responses for this variant\n",
    "    texts = df[df['variant'] == variant]['response_text'].tolist()\n",
    "    \n",
    "    # Combine all text\n",
    "    combined_text = ' '.join(texts).lower()\n",
    "    \n",
    "    # Extract words (alphanumeric only)\n",
    "    words = re.findall(r'\\b[a-z]+\\b', combined_text)\n",
    "    \n",
    "    # Filter by length and remove common stop words\n",
    "    stop_words = {'the', 'this', 'that', 'with', 'from', 'have', 'they', 'will', 'your', \n",
    "                  'more', 'about', 'which', 'their', 'there', 'than', 'them', 'these',\n",
    "                  'been', 'were', 'when', 'where', 'also', 'can', 'are', 'and', 'for'}\n",
    "    \n",
    "    words = [w for w in words if len(w) >= min_length and w not in stop_words]\n",
    "    \n",
    "    # Count and return top N\n",
    "    return Counter(words).most_common(n)\n",
    "\n",
    "# Analyze top words for each variant\n",
    "print(\"Top 15 Words by Variant:\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for variant in df_variants['variant'].unique():\n",
    "    print(f\"\\n{variant}:\")\n",
    "    top_words = get_top_words(df_variants, variant, n=15)\n",
    "    \n",
    "    for word, count in top_words:\n",
    "        print(f\"  {word}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualize Word Frequency Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a comparison of top words across variants\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, variant in enumerate(df_variants['variant'].unique()):\n",
    "    if idx >= len(axes):\n",
    "        break\n",
    "        \n",
    "    top_words = get_top_words(df_variants, variant, n=10)\n",
    "    words, counts = zip(*top_words)\n",
    "    \n",
    "    axes[idx].barh(words, counts, color=sns.color_palette(\"husl\", 10))\n",
    "    axes[idx].set_xlabel('Frequency')\n",
    "    axes[idx].set_title(f'Top Words: {variant}')\n",
    "    axes[idx].invert_yaxis()\n",
    "\n",
    "# Hide any unused subplots\n",
    "for idx in range(len(df_variants['variant'].unique()), len(axes)):\n",
    "    axes[idx].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('word_frequency_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all results\n",
    "analyzer.save_results(df_variants, \"prompt_variants_results\")\n",
    "analyzer.save_results(df_temperature, \"temperature_analysis_results\")\n",
    "\n",
    "print(\"Results saved!\")\n",
    "print(\"\\nGenerated files:\")\n",
    "print(\"  - prompt_variants_results.csv\")\n",
    "print(\"  - prompt_variants_results.json\")\n",
    "print(\"  - temperature_analysis_results.csv\")\n",
    "print(\"  - temperature_analysis_results.json\")\n",
    "print(\"  - prompt_comparison.png\")\n",
    "print(\"  - temperature_effects.png\")\n",
    "print(\"  - word_frequency_comparison.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. One-Shot vs Multi-Shot Prompting Analysis\n",
    "\n",
    "This section analyzes how the number of examples (zero-shot, one-shot, few-shot, multi-shot) affects:\n",
    "1. **Response Consistency (Stationarity)**: How stable and predictable are the outputs?\n",
    "2. **Format Adherence**: Does the model follow the format shown in examples?\n",
    "3. **Response Quality**: How well does it extract structured information?\n",
    "\n",
    "We'll test this with a product review classification task, extracting sentiment and key features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the task: Extract sentiment and key features from a product review\n",
    "test_review = \"\"\"I recently purchased the UltraBook Pro laptop and I'm thoroughly impressed. \n",
    "The battery life easily lasts 12 hours, the display is crystal clear, and the keyboard is comfortable \n",
    "for long typing sessions. However, the price point is quite high and it gets warm during intensive tasks.\"\"\"\n",
    "\n",
    "# Define variants with different numbers of examples\n",
    "shot_variants = {\n",
    "    \"zero_shot\": {\n",
    "        \"prompt\": f\"\"\"Analyze this product review and provide:\n",
    "1. Overall sentiment (Positive/Negative/Mixed)\n",
    "2. Key positive features (list)\n",
    "3. Key negative features (list)\n",
    "\n",
    "Review: {test_review}\"\"\",\n",
    "        \"system\": \"\"\n",
    "    },\n",
    "    \n",
    "    \"one_shot\": {\n",
    "        \"prompt\": f\"\"\"Analyze product reviews and extract sentiment and key features.\n",
    "\n",
    "Example:\n",
    "Review: \"The wireless headphones sound great and are very comfortable. Battery lasts 20 hours. \n",
    "But they're expensive and don't fold for storage.\"\n",
    "\n",
    "Analysis:\n",
    "1. Overall sentiment: Mixed\n",
    "2. Key positive features:\n",
    "   - Great sound quality\n",
    "   - Very comfortable\n",
    "   - 20-hour battery life\n",
    "3. Key negative features:\n",
    "   - Expensive\n",
    "   - Don't fold for storage\n",
    "\n",
    "Now analyze this review:\n",
    "Review: {test_review}\n",
    "\n",
    "Analysis:\"\"\",\n",
    "        \"system\": \"\"\n",
    "    },\n",
    "    \n",
    "    \"two_shot\": {\n",
    "        \"prompt\": f\"\"\"Analyze product reviews and extract sentiment and key features.\n",
    "\n",
    "Example 1:\n",
    "Review: \"The wireless headphones sound great and are very comfortable. Battery lasts 20 hours. \n",
    "But they're expensive and don't fold for storage.\"\n",
    "\n",
    "Analysis:\n",
    "1. Overall sentiment: Mixed\n",
    "2. Key positive features:\n",
    "   - Great sound quality\n",
    "   - Very comfortable\n",
    "   - 20-hour battery life\n",
    "3. Key negative features:\n",
    "   - Expensive\n",
    "   - Don't fold for storage\n",
    "\n",
    "Example 2:\n",
    "Review: \"This smartphone is amazing! Super fast processor, excellent camera, beautiful design. \n",
    "Absolutely love it!\"\n",
    "\n",
    "Analysis:\n",
    "1. Overall sentiment: Positive\n",
    "2. Key positive features:\n",
    "   - Super fast processor\n",
    "   - Excellent camera\n",
    "   - Beautiful design\n",
    "3. Key negative features:\n",
    "   - None mentioned\n",
    "\n",
    "Now analyze this review:\n",
    "Review: {test_review}\n",
    "\n",
    "Analysis:\"\"\",\n",
    "        \"system\": \"\"\n",
    "    },\n",
    "    \n",
    "    \"multi_shot\": {\n",
    "        \"prompt\": f\"\"\"Analyze product reviews and extract sentiment and key features.\n",
    "\n",
    "Example 1:\n",
    "Review: \"The wireless headphones sound great and are very comfortable. Battery lasts 20 hours. \n",
    "But they're expensive and don't fold for storage.\"\n",
    "\n",
    "Analysis:\n",
    "1. Overall sentiment: Mixed\n",
    "2. Key positive features:\n",
    "   - Great sound quality\n",
    "   - Very comfortable\n",
    "   - 20-hour battery life\n",
    "3. Key negative features:\n",
    "   - Expensive\n",
    "   - Don't fold for storage\n",
    "\n",
    "Example 2:\n",
    "Review: \"This smartphone is amazing! Super fast processor, excellent camera, beautiful design. \n",
    "Absolutely love it!\"\n",
    "\n",
    "Analysis:\n",
    "1. Overall sentiment: Positive\n",
    "2. Key positive features:\n",
    "   - Super fast processor\n",
    "   - Excellent camera\n",
    "   - Beautiful design\n",
    "3. Key negative features:\n",
    "   - None mentioned\n",
    "\n",
    "Example 3:\n",
    "Review: \"Terrible coffee maker. Leaked water everywhere, broke after 2 weeks. Complete waste of money.\"\n",
    "\n",
    "Analysis:\n",
    "1. Overall sentiment: Negative\n",
    "2. Key positive features:\n",
    "   - None mentioned\n",
    "3. Key negative features:\n",
    "   - Leaks water\n",
    "   - Poor durability (broke after 2 weeks)\n",
    "   - Poor value for money\n",
    "\n",
    "Example 4:\n",
    "Review: \"The fitness tracker works well for basic tracking. Steps and heart rate are accurate. \n",
    "However, the app is clunky and syncing is unreliable.\"\n",
    "\n",
    "Analysis:\n",
    "1. Overall sentiment: Mixed\n",
    "2. Key positive features:\n",
    "   - Accurate step tracking\n",
    "   - Accurate heart rate monitoring\n",
    "3. Key negative features:\n",
    "   - Clunky app interface\n",
    "   - Unreliable syncing\n",
    "\n",
    "Now analyze this review:\n",
    "Review: {test_review}\n",
    "\n",
    "Analysis:\"\"\",\n",
    "        \"system\": \"\"\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Few-Shot Variants Defined:\")\n",
    "print(f\"  - zero_shot: 0 examples\")\n",
    "print(f\"  - one_shot: 1 example\")\n",
    "print(f\"  - two_shot: 2 examples\")\n",
    "print(f\"  - multi_shot: 4 examples\")\n",
    "print(f\"\\nTest review: {test_review[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Few-Shot Experiment\n",
    "\n",
    "We'll run each variant multiple times to measure consistency (stationarity) across responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the experiment - test each variant multiple times\n",
    "print(\"Running few-shot analysis... This will take a few minutes.\\n\")\n",
    "\n",
    "df_few_shot = analyzer.compare_prompts(\n",
    "    base_question=test_review,\n",
    "    prompt_variants=shot_variants,\n",
    "    temperature=0.7,  # Using temperature 0.7 for some diversity\n",
    "    num_samples=15  # More samples to better measure stationarity\n",
    ")\n",
    "\n",
    "print(\"\\n✓ Few-shot analysis complete!\")\n",
    "print(f\"\\nTotal responses collected: {len(df_few_shot)}\")\n",
    "print(f\"Variants tested: {df_few_shot['variant'].unique().tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze Prompt Stationarity\n",
    "\n",
    "**Stationarity** refers to how consistent and predictable the model's responses are. \n",
    "High stationarity means the model produces similar outputs given the same prompt (desirable for production use).\n",
    "\n",
    "We'll measure:\n",
    "1. **Response Variance**: Standard deviation in response length and word count\n",
    "2. **Format Consistency**: How often responses follow the expected format\n",
    "3. **Uniqueness Ratio**: Proportion of unique responses (lower = more stationary)\n",
    "4. **Content Consistency**: Similarity in key terms used across responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_stationarity(df, text_column='response_text'):\n",
    "    \"\"\"\n",
    "    Analyze prompt stationarity - how consistent are the responses?\n",
    "    \n",
    "    Returns metrics for each variant:\n",
    "    - Response length variance (lower = more stationary)\n",
    "    - Word count variance (lower = more stationary)\n",
    "    - Uniqueness ratio (lower = more stationary)\n",
    "    - Format consistency (higher = more stationary)\n",
    "    - Coefficient of variation for length (normalized measure)\n",
    "    \"\"\"\n",
    "    stationarity_metrics = {}\n",
    "    \n",
    "    for variant in df['variant'].unique():\n",
    "        variant_df = df[df['variant'] == variant]\n",
    "        responses = variant_df[text_column].tolist()\n",
    "        \n",
    "        # Basic stats\n",
    "        lengths = variant_df['response_length'].values\n",
    "        word_counts = variant_df['word_count'].values\n",
    "        \n",
    "        # Uniqueness\n",
    "        unique_responses = len(set(responses))\n",
    "        total_responses = len(responses)\n",
    "        uniqueness_ratio = unique_responses / total_responses\n",
    "        \n",
    "        # Variance metrics\n",
    "        length_std = np.std(lengths)\n",
    "        length_mean = np.mean(lengths)\n",
    "        word_count_std = np.std(word_counts)\n",
    "        word_count_mean = np.mean(word_counts)\n",
    "        \n",
    "        # Coefficient of variation (CV) - normalized measure of dispersion\n",
    "        # Lower CV = more stationary\n",
    "        length_cv = (length_std / length_mean) if length_mean > 0 else 0\n",
    "        word_count_cv = (word_count_std / word_count_mean) if word_count_mean > 0 else 0\n",
    "        \n",
    "        # Format consistency - check if responses follow numbered list format\n",
    "        format_matches = 0\n",
    "        for response in responses:\n",
    "            # Check if response contains \"1.\" and \"2.\" and \"3.\"\n",
    "            if all(f\"{i}.\" in response for i in [1, 2, 3]):\n",
    "                format_matches += 1\n",
    "        format_consistency = format_matches / total_responses\n",
    "        \n",
    "        # Content consistency - measure overlap in key terms\n",
    "        # Extract key terms from all responses\n",
    "        all_terms = []\n",
    "        for response in responses:\n",
    "            # Simple term extraction (lowercased words)\n",
    "            terms = set(response.lower().split())\n",
    "            all_terms.append(terms)\n",
    "        \n",
    "        # Calculate average Jaccard similarity between all pairs\n",
    "        if len(all_terms) > 1:\n",
    "            similarities = []\n",
    "            for i in range(len(all_terms)):\n",
    "                for j in range(i + 1, len(all_terms)):\n",
    "                    intersection = len(all_terms[i] & all_terms[j])\n",
    "                    union = len(all_terms[i] | all_terms[j])\n",
    "                    similarity = intersection / union if union > 0 else 0\n",
    "                    similarities.append(similarity)\n",
    "            avg_content_similarity = np.mean(similarities) if similarities else 0\n",
    "        else:\n",
    "            avg_content_similarity = 1.0\n",
    "        \n",
    "        stationarity_metrics[variant] = {\n",
    "            'uniqueness_ratio': uniqueness_ratio,\n",
    "            'length_std': length_std,\n",
    "            'length_cv': length_cv,\n",
    "            'word_count_std': word_count_std,\n",
    "            'word_count_cv': word_count_cv,\n",
    "            'format_consistency': format_consistency,\n",
    "            'content_similarity': avg_content_similarity,\n",
    "            'num_samples': total_responses,\n",
    "            # Lower stationarity_score = more stationary\n",
    "            # Combine multiple metrics (normalized)\n",
    "            'stationarity_score': (uniqueness_ratio + length_cv + word_count_cv) / 3 - (format_consistency + avg_content_similarity) / 2\n",
    "        }\n",
    "    \n",
    "    return stationarity_metrics\n",
    "\n",
    "# Analyze stationarity\n",
    "print(\"Analyzing prompt stationarity...\\n\")\n",
    "stationarity_metrics = analyze_stationarity(df_few_shot)\n",
    "\n",
    "# Convert to DataFrame for better visualization\n",
    "stationarity_df = pd.DataFrame(stationarity_metrics).T\n",
    "stationarity_df = stationarity_df.round(4)\n",
    "\n",
    "# Sort by number of examples (0, 1, 2, 4)\n",
    "shot_order = ['zero_shot', 'one_shot', 'two_shot', 'multi_shot']\n",
    "stationarity_df = stationarity_df.reindex(shot_order)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"STATIONARITY METRICS BY VARIANT\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nLower values indicate MORE stationary (more consistent) responses\")\n",
    "print(\"Higher values indicate LESS stationary (more diverse) responses\\n\")\n",
    "print(stationarity_df)\n",
    "\n",
    "# Highlight key findings\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"KEY FINDINGS:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "most_stationary = stationarity_df['stationarity_score'].idxmin()\n",
    "least_stationary = stationarity_df['stationarity_score'].idxmax()\n",
    "\n",
    "print(f\"\\nMost Stationary (most consistent): {most_stationary}\")\n",
    "print(f\"  - Stationarity Score: {stationarity_df.loc[most_stationary, 'stationarity_score']:.4f}\")\n",
    "print(f\"  - Uniqueness Ratio: {stationarity_df.loc[most_stationary, 'uniqueness_ratio']:.2%}\")\n",
    "print(f\"  - Format Consistency: {stationarity_df.loc[most_stationary, 'format_consistency']:.2%}\")\n",
    "print(f\"  - Content Similarity: {stationarity_df.loc[most_stationary, 'content_similarity']:.2%}\")\n",
    "\n",
    "print(f\"\\nLeast Stationary (most diverse): {least_stationary}\")\n",
    "print(f\"  - Stationarity Score: {stationarity_df.loc[least_stationary, 'stationarity_score']:.4f}\")\n",
    "print(f\"  - Uniqueness Ratio: {stationarity_df.loc[least_stationary, 'uniqueness_ratio']:.2%}\")\n",
    "print(f\"  - Format Consistency: {stationarity_df.loc[least_stationary, 'format_consistency']:.2%}\")\n",
    "print(f\"  - Content Similarity: {stationarity_df.loc[least_stationary, 'content_similarity']:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Sample Responses\n",
    "\n",
    "Let's look at a few sample responses from each variant to see the differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display 2 sample responses from each variant\n",
    "for variant in shot_order:\n",
    "    variant_responses = df_few_shot[df_few_shot['variant'] == variant]['response_text'].tolist()[:2]\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(f\"VARIANT: {variant.upper()} ({variant.split('_')[0]} examples)\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for idx, response in enumerate(variant_responses, 1):\n",
    "        print(f\"\\nSample {idx}:\")\n",
    "        print(\"-\" * 80)\n",
    "        print(response)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Stationarity Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualizations for stationarity analysis\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "# 1. Stationarity Score (lower = more stationary)\n",
    "ax = axes[0, 0]\n",
    "colors = sns.color_palette(\"RdYlGn_r\", len(shot_order))\n",
    "bars = ax.bar(shot_order, stationarity_df['stationarity_score'], color=colors)\n",
    "ax.set_xlabel('Variant (Number of Examples)')\n",
    "ax.set_ylabel('Stationarity Score (lower = more consistent)')\n",
    "ax.set_title('Overall Stationarity Score by Variant')\n",
    "ax.set_xticklabels(['0-shot', '1-shot', '2-shot', '4-shot'])\n",
    "ax.axhline(y=0, color='black', linestyle='--', alpha=0.3)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 2. Uniqueness Ratio (lower = more stationary)\n",
    "ax = axes[0, 1]\n",
    "bars = ax.bar(shot_order, stationarity_df['uniqueness_ratio'], color=sns.color_palette(\"Reds_r\", len(shot_order)))\n",
    "ax.set_xlabel('Variant (Number of Examples)')\n",
    "ax.set_ylabel('Uniqueness Ratio (lower = more consistent)')\n",
    "ax.set_title('Response Uniqueness by Variant')\n",
    "ax.set_xticklabels(['0-shot', '1-shot', '2-shot', '4-shot'])\n",
    "ax.set_ylim(0, 1.1)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 3. Format Consistency (higher = more stationary)\n",
    "ax = axes[0, 2]\n",
    "bars = ax.bar(shot_order, stationarity_df['format_consistency'], color=sns.color_palette(\"Greens\", len(shot_order)))\n",
    "ax.set_xlabel('Variant (Number of Examples)')\n",
    "ax.set_ylabel('Format Consistency (higher = better)')\n",
    "ax.set_title('Format Adherence by Variant')\n",
    "ax.set_xticklabels(['0-shot', '1-shot', '2-shot', '4-shot'])\n",
    "ax.set_ylim(0, 1.1)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 4. Content Similarity (higher = more stationary)\n",
    "ax = axes[1, 0]\n",
    "bars = ax.bar(shot_order, stationarity_df['content_similarity'], color=sns.color_palette(\"Blues\", len(shot_order)))\n",
    "ax.set_xlabel('Variant (Number of Examples)')\n",
    "ax.set_ylabel('Content Similarity (higher = more consistent)')\n",
    "ax.set_title('Average Content Similarity by Variant')\n",
    "ax.set_xticklabels(['0-shot', '1-shot', '2-shot', '4-shot'])\n",
    "ax.set_ylim(0, 1.1)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 5. Response Length Coefficient of Variation (lower = more stationary)\n",
    "ax = axes[1, 1]\n",
    "bars = ax.bar(shot_order, stationarity_df['length_cv'], color=sns.color_palette(\"Oranges_r\", len(shot_order)))\n",
    "ax.set_xlabel('Variant (Number of Examples)')\n",
    "ax.set_ylabel('Length CV (lower = more consistent)')\n",
    "ax.set_title('Response Length Variability')\n",
    "ax.set_xticklabels(['0-shot', '1-shot', '2-shot', '4-shot'])\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 6. Response Length Distribution by Variant\n",
    "ax = axes[1, 2]\n",
    "for variant in shot_order:\n",
    "    variant_data = df_few_shot[df_few_shot['variant'] == variant]['response_length']\n",
    "    ax.boxplot([variant_data], positions=[shot_order.index(variant)], widths=0.6, patch_artist=True,\n",
    "                boxprops=dict(facecolor=colors[shot_order.index(variant)], alpha=0.7))\n",
    "\n",
    "ax.set_xlabel('Variant (Number of Examples)')\n",
    "ax.set_ylabel('Response Length (characters)')\n",
    "ax.set_title('Response Length Distribution')\n",
    "ax.set_xticks(range(len(shot_order)))\n",
    "ax.set_xticklabels(['0-shot', '1-shot', '2-shot', '4-shot'])\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('few_shot_stationarity_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Visualization saved as 'few_shot_stationarity_analysis.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Few-Shot Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the few-shot results\n",
    "analyzer.save_results(df_few_shot, \"few_shot_results\")\n",
    "\n",
    "# Also save the stationarity metrics\n",
    "stationarity_df.to_csv(\"stationarity_metrics.csv\")\n",
    "stationarity_df.to_json(\"stationarity_metrics.json\", orient='index', indent=2)\n",
    "\n",
    "print(\"Results saved!\")\n",
    "print(\"\\nGenerated files:\")\n",
    "print(\"  - few_shot_results.csv\")\n",
    "print(\"  - few_shot_results.json\")\n",
    "print(\"  - stationarity_metrics.csv\")\n",
    "print(\"  - stationarity_metrics.json\")\n",
    "print(\"  - few_shot_stationarity_analysis.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Insights: Impact of Examples on Prompt Stationarity\n",
    "\n",
    "**What we learned:**\n",
    "\n",
    "1. **More examples generally increase stationarity** - Few-shot prompts (especially multi-shot with 4 examples) tend to produce more consistent, predictable outputs.\n",
    "\n",
    "2. **Format consistency improves with examples** - When you show the model the desired format through examples, it's more likely to follow that format consistently.\n",
    "\n",
    "3. **Content similarity increases with examples** - More examples help the model converge on similar content and vocabulary across multiple runs.\n",
    "\n",
    "4. **Trade-off: Consistency vs. Creativity** \n",
    "   - Zero-shot: More diverse, creative responses but less predictable\n",
    "   - Few-shot: More consistent, reliable responses but potentially less creative\n",
    "\n",
    "5. **Practical implications:**\n",
    "   - For **production systems** where reliability matters: Use few-shot or multi-shot prompts\n",
    "   - For **creative applications** where diversity is valuable: Zero-shot or one-shot may be better\n",
    "   - For **structured extraction tasks**: Multi-shot prompts significantly improve format adherence\n",
    "\n",
    "6. **Diminishing returns** - Going from 2 to 4 examples may show smaller improvements than going from 0 to 1 example. Test to find the optimal number for your use case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Your Custom Experiments\n",
    "\n",
    "Use this section to run your own experiments!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your own question and variants here\n",
    "\n",
    "my_question = \"Your question here\"\n",
    "\n",
    "my_variants = {\n",
    "    \"variant_1\": {\n",
    "        \"prompt\": my_question,\n",
    "        \"system\": \"\"\n",
    "    },\n",
    "    # Add more variants...\n",
    "}\n",
    "\n",
    "# Run your experiment\n",
    "# my_results = analyzer.compare_prompts(my_question, my_variants, temperature=0.7, num_samples=5)\n",
    "# analyzer.visualize_prompt_comparison(my_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "1. **Prompt Engineering Techniques**: How different prompting strategies (zero-shot, few-shot, chain-of-thought, etc.) affect outputs\n",
    "2. **Temperature Effects**: How temperature influences output diversity and creativity\n",
    "3. **Output Analysis**: Measuring response length, diversity, and patterns\n",
    "4. **Word Frequency**: Understanding vocabulary usage across different prompts\n",
    "5. **Few-Shot Learning Impact on Stationarity**: How the number of examples affects response consistency and predictability\n",
    "\n",
    "Key findings you might observe:\n",
    "- Lower temperatures (0.0-0.3) produce more consistent, deterministic outputs\n",
    "- Higher temperatures (1.0-1.5) increase diversity but may reduce coherence\n",
    "- Structured prompts tend to produce more consistent formatting\n",
    "- System prompts can significantly influence tone and style\n",
    "- Few-shot examples guide the model toward specific response patterns\n",
    "- **More examples increase stationarity**: Multi-shot prompts (4 examples) produce the most consistent, predictable outputs\n",
    "- **Format adherence improves with examples**: Few-shot learning significantly improves structured output compliance\n",
    "- **Trade-off between consistency and creativity**: Zero-shot prompts are more creative but less reliable; multi-shot prompts are more consistent but potentially less diverse"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
